{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PXBZWxPdrUVT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbMZ_JlmrXSO",
        "outputId": "c467cd87-77f5-4ae1-9bf3-53379536b7e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path1 = '/content/drive/MyDrive/VU/ANN/encoded_data.csv'\n",
        "path2 = '/content/drive/MyDrive/VU/ANN/prepared_test_data.csv'\n",
        "train_df = pd.read_csv(path1)\n",
        "test_df = pd.read_csv(path2)"
      ],
      "metadata": {
        "id": "rp-rgIgOre66"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rearrange data to time windowed series for TCN input"
      ],
      "metadata": {
        "id": "3pEm9QqmzdGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pop out target data and id's for future predictions:\n",
        "Y_train = train_df.iloc[:, -1]\n",
        "ids = train_df.iloc[:, -2]\n",
        "train_df = train_df.drop('id', axis=1)"
      ],
      "metadata": {
        "id": "-4K3iaLGvUrc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the DataFrame to a numpy array\n",
        "encoded_historical_array = train_df.values           # dataframe is order by each product & store pair that has a a historical sequence of 1713 samples\n",
        "# Convert the numpy array to a PyTorch tensor\n",
        "encoded_historical_tensor = torch.tensor(encoded_historical_array, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "UIMpyjwRsoCd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 32  # sequence length\n",
        "P = 16  # prediction length\n",
        "\n",
        "sequences = []  # of channels\n",
        "targets = []\n",
        "test_sequences = []\n",
        "\n",
        "# Each store-item has 1713 samples\n",
        "samples_per_combination = 1713\n",
        "\n",
        "# For each unique historical sequence of (product,store) touple:\n",
        "for i in range(0, len(encoded_historical_tensor), samples_per_combination):\n",
        "    # Make a sequence with sliding window & it's corresponding targets\n",
        "  for j in range(i, i + samples_per_combination - (N + P) + 1):\n",
        "        sequences.append(encoded_historical_tensor[j:j+N])\n",
        "        targets.append(Y_train.values[j+N:j+N+P])\n",
        "  # And gather the last N window for test set:\n",
        "  if i == 0:\n",
        "    test_sequences.append(encoded_historical_tensor[samples_per_combination-N:samples_per_combination])\n",
        "  else:\n",
        "    test_sequences.append(encoded_historical_tensor[i+samples_per_combination-N:i+samples_per_combination])\n",
        "\n",
        "\n",
        "# Convert train sequences, targets & test sequence to tensors:\n",
        "sequences_tensor = torch.stack(sequences).transpose(1, 2)\n",
        "targets_tensor = torch.tensor(np.array(targets)).float()\n",
        "test_sequences = torch.stack(test_sequences).transpose(1, 2)\n",
        "\n",
        "\n",
        "# Create a new Dataset\n",
        "tcn_dataset = TensorDataset(sequences_tensor, targets_tensor)\n",
        "\n",
        "# DataLoader for TCN\n",
        "optimal_num_workers = 2\n",
        "tcn_dataloader = DataLoader(tcn_dataset, batch_size=(1713-N)*33, shuffle=True, num_workers=optimal_num_workers)"
      ],
      "metadata": {
        "id": "5nbhYzs2sIDv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear up RAM:\n",
        "gc.collect()\n",
        "\n",
        "del train_df\n",
        "del test_df\n",
        "del encoded_historical_array\n",
        "del sequences\n",
        "del targets"
      ],
      "metadata": {
        "id": "LGoh7gOeJxYm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model & training:"
      ],
      "metadata": {
        "id": "CoI6VyiLyUH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TCNBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, dilation):\n",
        "        \"\"\"\n",
        "        Initializes the TCN block.\n",
        "\n",
        "        Parameters:\n",
        "            in_channels (int): Number of input channels.\n",
        "            out_channels (int): Number of output channels.\n",
        "            kernel_size (int): Size of the convolutional kernel.\n",
        "            dilation (int): Dilation rate for the convolution, providing the ability to capture long-range dependencies.\n",
        "        \"\"\"\n",
        "        super(TCNBlock, self).__init__()\n",
        "        padding = (kernel_size - 1) * dilation\n",
        "\n",
        "        # Convolutional layer\n",
        "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation)\n",
        "\n",
        "        # Batch normalization layer\n",
        "        self.bn = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "        # Activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Skip connection\n",
        "        self.skip_connection = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the TCN block. Implements causal convolution by removing the excess padding from the right.\n",
        "\n",
        "        Parameters:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, sequence_length).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after applying the block.\n",
        "        \"\"\"\n",
        "        # Causal convolution\n",
        "        out = self.conv(x)\n",
        "        out = out[:, :, :-self.conv.padding[0]]\n",
        "\n",
        "        # Apply skip connection\n",
        "        identity = x if self.skip_connection is None else self.skip_connection(x)\n",
        "        out += identity  # Element-wise addition\n",
        "\n",
        "        return self.relu(out)\n",
        "\n",
        "class TCN(nn.Module):\n",
        "    \"\"\"\n",
        "    Temporal Convolutional Network (TCN) architecture.\n",
        "\n",
        "    Attributes:\n",
        "        network (nn.Sequential): Sequence of TCN blocks.\n",
        "        fc (nn.Linear): Fully connected layer for producing the final predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size, num_channels, kernel_size=2):\n",
        "        \"\"\"\n",
        "        Initializes the TCN.\n",
        "\n",
        "        Parameters:\n",
        "            input_size (int): Number of input channels.\n",
        "            output_size (int): Number of output values to predict (in our case, 16).\n",
        "            num_channels (list): List of output channels for each TCN block.\n",
        "            kernel_size (int, optional): Size of the convolutional kernel. Defaults to 2.\n",
        "        \"\"\"\n",
        "        super(TCN, self).__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        # Set up convolutional layers\n",
        "        for i in range(num_levels):\n",
        "            dilation = 2 ** i\n",
        "            in_channels = input_size if i == 0 else num_channels[i-1]\n",
        "            out_channels = num_channels[i]\n",
        "            layers.append(TCNBlock(in_channels, out_channels, kernel_size, dilation))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "        self.fc = nn.Linear(num_channels[-1], output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the TCN.\n",
        "\n",
        "        Parameters:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, input_size, sequence_length).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Predictions of shape (batch_size, output_size).\n",
        "        \"\"\"\n",
        "        x = self.network(x)\n",
        "        x = self.fc(x[:, :, -1])\n",
        "        return F.relu(x)"
      ],
      "metadata": {
        "id": "dISVJbIHyWfa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model Initialization\n",
        "input_size = len(encoded_historical_tensor[2])  # Number of features\n",
        "del encoded_historical_tensor\n",
        "output_size = P  # Number of days to predict\n",
        "num_channels = [32,32,16,16,16]  # Adjust for the desired network depth and width\n",
        "model = TCN(input_size, output_size, num_channels).to(device)"
      ],
      "metadata": {
        "id": "K-CV17x-CiQG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function & Optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 15\n",
        "print_every_n_epochs = 1  # Print updates every epoch\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(tcn_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "        # loss_value = rmsle(target, outputs)\n",
        "        # loss = torch.tensor(loss_value, requires_grad=True).to(device)  # Convert back to tensor and move to device\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print loss information every 'print_every_n_epochs' epochs\n",
        "    if epoch % print_every_n_epochs == 0:\n",
        "        epoch_loss = running_loss / len(tcn_dataloader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f}\")\n",
        "        if epoch_loss <= 206000.6939:\n",
        "          break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sfzA1yFBRa4",
        "outputId": "f81886fb-32b0-43c0-a075-59cf6a5f284c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15] Loss: 1114368.2303\n",
            "Epoch [2/15] Loss: 597196.4549\n",
            "Epoch [3/15] Loss: 484267.4705\n",
            "Epoch [4/15] Loss: 476829.8848\n",
            "Epoch [5/15] Loss: 472080.1933\n",
            "Epoch [6/15] Loss: 467579.2043\n",
            "Epoch [7/15] Loss: 416489.0689\n",
            "Epoch [8/15] Loss: 249598.6004\n",
            "Epoch [9/15] Loss: 234043.7054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "torch.save(model.state_dict(), 'tcn_model_weights.pth')"
      ],
      "metadata": {
        "id": "eRLwOYLGWE78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Test:"
      ],
      "metadata": {
        "id": "28RVFWxQrpDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del Y_train\n",
        "#del tcn_dataloader"
      ],
      "metadata": {
        "id": "652WCJwdtQfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('tcn_model_weights.pth'))\n",
        "\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# List to store predictions\n",
        "predictions = []\n",
        "\n",
        "# Use no_grad to ensure gradient isn't computed\n",
        "with torch.no_grad():\n",
        "\n",
        "    # Batch predictions\n",
        "    test_loader = DataLoader(test_sequences, batch_size=100, shuffle=False)\n",
        "\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)  # Send to device\n",
        "        prediction = model(batch)\n",
        "        predictions.extend(prediction.cpu().numpy())\n",
        "\n",
        "# Convert predictions to a numpy array\n",
        "predictions_array = np.array(predictions)\n",
        "\n",
        "# Convert numpy array to DataFrame\n",
        "predictions_df = pd.DataFrame(predictions_array)\n"
      ],
      "metadata": {
        "id": "u2PV3M3cRcWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df.shape"
      ],
      "metadata": {
        "id": "HD_0d_6RzIZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty array for reordered predictions\n",
        "reordered_array = np.zeros((predictions_array.shape[0] * 16,))\n",
        "\n",
        "# Distribute the predictions based on the described pattern\n",
        "for idx in range(predictions_array.shape[0]):     # iterating over each store & product pair\n",
        "    for i in range(16):\n",
        "        position = idx + i * 1782    # shifting and repeating based on day\n",
        "        reordered_array[position] = predictions_array[idx, i]\n",
        "\n",
        "# Convert reordered array to a DataFrame\n",
        "reordered_predictions_df = pd.DataFrame(reordered_array, columns=['sales'])\n",
        "\n",
        "# Set index to start from 3000888\n",
        "reordered_predictions_df.index = np.arange(3000888, 3000888 + len(reordered_predictions_df))\n",
        "\n",
        "# Rename the index\n",
        "reordered_predictions_df.index.name = 'id'\n",
        "reordered_predictions_df.reset_index(inplace=True)\n",
        "\n",
        "# Ceil negative values\n",
        "reordered_predictions_df.loc[reordered_predictions_df['sales'] <= 0, 'sales'] = 0\n",
        "\n",
        "\n",
        "reordered_predictions_df.head()"
      ],
      "metadata": {
        "id": "4Lb2k70izKhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading the final predictions form for Kaggle"
      ],
      "metadata": {
        "id": "1lZPwj-bM6y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reordered_predictions_df.to_csv('/content/drive/MyDrive/VU/ANN/TCN_submission_second.csv', index=False)"
      ],
      "metadata": {
        "id": "mwf_DME7_4RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reordered_predictions_df['sales'] = reordered_predictions_df['sales'].round(0)\n",
        "reordered_predictions_df.to_csv('/content/drive/MyDrive/VU/ANN/TCN_submission_second_round.csv', index=False)"
      ],
      "metadata": {
        "id": "jhw119XmM59f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}